{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from glas_data_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import getopt\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import netrc\n",
    "import os.path\n",
    "import ssl\n",
    "import sys\n",
    "import time\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from urllib.parse import urlparse\n",
    "    from urllib.request import (\n",
    "        urlopen,\n",
    "        Request,\n",
    "        build_opener,\n",
    "        HTTPCookieProcessor,\n",
    "    )\n",
    "    from urllib.error import HTTPError, URLError\n",
    "except ImportError:\n",
    "    from urlparse import urlparse\n",
    "    from urllib2 import (\n",
    "        urlopen,\n",
    "        Request,\n",
    "        HTTPError,\n",
    "        URLError,\n",
    "        build_opener,\n",
    "        HTTPCookieProcessor,\n",
    "    )\n",
    "\n",
    "short_name = \"GLAH14\"\n",
    "version = \"034\"\n",
    "bounding_box = \"-125,45,-115,49.5\"\n",
    "\n",
    "time_start = \"2003-02-20T00:00:00Z\"\n",
    "time_end = \"2009-10-11T23:59:59Z\"\n",
    "polygon = \"\"\n",
    "filename_filter = \"\"\n",
    "url_list = []\n",
    "\n",
    "CMR_URL = \"https://cmr.earthdata.nasa.gov\"\n",
    "URS_URL = \"https://urs.earthdata.nasa.gov\"\n",
    "CMR_PAGE_SIZE = 2000\n",
    "CMR_FILE_URL = (\n",
    "    \"{0}/search/granules.json?provider=NSIDC_ECS\"\n",
    "    \"&sort_key[]=start_date&sort_key[]=producer_granule_id\"\n",
    "    \"&scroll=true&page_size={1}\".format(CMR_URL, CMR_PAGE_SIZE)\n",
    ")\n",
    "\n",
    "\n",
    "def get_username():\n",
    "    username = \"\"\n",
    "\n",
    "    # For Python 2/3 compatibility:\n",
    "    try:\n",
    "        do_input = raw_input  # noqa\n",
    "    except NameError:\n",
    "        do_input = input\n",
    "\n",
    "    while not username:\n",
    "        username = do_input(\"Earthdata username: \")\n",
    "    return username\n",
    "\n",
    "\n",
    "def get_password():\n",
    "    password = \"\"\n",
    "    while not password:\n",
    "        password = getpass(\"password: \")\n",
    "    return password\n",
    "\n",
    "\n",
    "def get_credentials(url):\n",
    "    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n",
    "    credentials = None\n",
    "    errprefix = \"\"\n",
    "    try:\n",
    "        info = netrc.netrc()\n",
    "        username, account, password = info.authenticators(\n",
    "            urlparse(URS_URL).hostname\n",
    "        )\n",
    "        errprefix = \"netrc error: \"\n",
    "    except Exception as e:\n",
    "        if not (\"No such file\" in str(e)):\n",
    "            print(\"netrc error: {0}\".format(str(e)))\n",
    "        username = None\n",
    "        password = None\n",
    "\n",
    "    while not credentials:\n",
    "        if not username:\n",
    "            username = get_username()\n",
    "            password = get_password()\n",
    "        credentials = \"{0}:{1}\".format(username, password)\n",
    "        credentials = base64.b64encode(credentials.encode(\"ascii\")).decode(\n",
    "            \"ascii\"\n",
    "        )\n",
    "\n",
    "        if url:\n",
    "            try:\n",
    "                req = Request(url)\n",
    "                req.add_header(\"Authorization\", \"Basic {0}\".format(credentials))\n",
    "                opener = build_opener(HTTPCookieProcessor())\n",
    "                opener.open(req)\n",
    "            except HTTPError:\n",
    "                print(errprefix + \"Incorrect username or password\")\n",
    "                errprefix = \"\"\n",
    "                credentials = None\n",
    "                username = None\n",
    "                password = None\n",
    "\n",
    "    return credentials\n",
    "\n",
    "\n",
    "def build_version_query_params(version):\n",
    "    desired_pad_length = 3\n",
    "    if len(version) > desired_pad_length:\n",
    "        print('Version string too long: \"{0}\"'.format(version))\n",
    "        quit()\n",
    "\n",
    "    version = str(int(version))  # Strip off any leading zeros\n",
    "    query_params = \"\"\n",
    "\n",
    "    while len(version) <= desired_pad_length:\n",
    "        padded_version = version.zfill(desired_pad_length)\n",
    "        query_params += \"&version={0}\".format(padded_version)\n",
    "        desired_pad_length -= 1\n",
    "    return query_params\n",
    "\n",
    "\n",
    "def filter_add_wildcards(filter):\n",
    "    if not filter.startswith(\"*\"):\n",
    "        filter = \"*\" + filter\n",
    "    if not filter.endswith(\"*\"):\n",
    "        filter = filter + \"*\"\n",
    "    return filter\n",
    "\n",
    "\n",
    "def build_filename_filter(filename_filter):\n",
    "    filters = filename_filter.split(\",\")\n",
    "    result = \"&options[producer_granule_id][pattern]=true\"\n",
    "    for filter in filters:\n",
    "        result += \"&producer_granule_id[]=\" + filter_add_wildcards(filter)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_cmr_query_url(\n",
    "    short_name,\n",
    "    version,\n",
    "    time_start,\n",
    "    time_end,\n",
    "    bounding_box=None,\n",
    "    polygon=None,\n",
    "    filename_filter=None,\n",
    "):\n",
    "    params = \"&short_name={0}\".format(short_name)\n",
    "    params += build_version_query_params(version)\n",
    "    params += \"&temporal[]={0},{1}\".format(time_start, time_end)\n",
    "    if polygon:\n",
    "        params += \"&polygon={0}\".format(polygon)\n",
    "    elif bounding_box:\n",
    "        params += \"&bounding_box={0}\".format(bounding_box)\n",
    "    if filename_filter:\n",
    "        params += build_filename_filter(filename_filter)\n",
    "    return CMR_FILE_URL + params\n",
    "\n",
    "\n",
    "def get_speed(time_elapsed, chunk_size):\n",
    "    if time_elapsed <= 0:\n",
    "        return \"\"\n",
    "    speed = chunk_size / time_elapsed\n",
    "    if speed <= 0:\n",
    "        speed = 1\n",
    "    size_name = (\"\", \"k\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\")\n",
    "    i = int(math.floor(math.log(speed, 1000)))\n",
    "    p = math.pow(1000, i)\n",
    "    return \"{0:.1f}{1}B/s\".format(speed / p, size_name[i])\n",
    "\n",
    "\n",
    "def output_progress(count, total, status=\"\", bar_len=60):\n",
    "    if total <= 0:\n",
    "        return\n",
    "    fraction = min(max(count / float(total), 0), 1)\n",
    "    filled_len = int(round(bar_len * fraction))\n",
    "    percents = int(round(100.0 * fraction))\n",
    "    bar = \"=\" * filled_len + \" \" * (bar_len - filled_len)\n",
    "    fmt = \"  [{0}] {1:3d}%  {2}   \".format(bar, percents, status)\n",
    "    print(\"\\b\" * (len(fmt) + 4), end=\"\")  # clears the line\n",
    "    sys.stdout.write(fmt)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n",
    "    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n",
    "    while True:\n",
    "        data = file_object.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n",
    "\n",
    "\n",
    "def cmr_download(urls, credentials=None, force=False, quiet=False):\n",
    "    \"\"\"Download files from list of urls.\"\"\"\n",
    "    if not urls:\n",
    "        return\n",
    "\n",
    "    url_count = len(urls)\n",
    "    if not quiet:\n",
    "        print(\"Downloading {0} files...\".format(url_count))\n",
    "\n",
    "    for index, url in enumerate(urls, start=1):\n",
    "        if not credentials and urlparse(url).scheme == \"https\":\n",
    "            credentials = get_credentials(url)\n",
    "\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        if not quiet:\n",
    "            print(\n",
    "                \"{0}/{1}: {2}\".format(\n",
    "                    str(index).zfill(len(str(url_count))), url_count, filename\n",
    "                )\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            req = Request(url)\n",
    "            if credentials:\n",
    "                req.add_header(\"Authorization\", \"Basic {0}\".format(credentials))\n",
    "            opener = build_opener(HTTPCookieProcessor())\n",
    "            response = opener.open(req)\n",
    "            length = int(response.headers[\"content-length\"])\n",
    "            try:\n",
    "                if not force and length == os.path.getsize(filename):\n",
    "                    if not quiet:\n",
    "                        print(\"  File exists, skipping\")\n",
    "                    continue\n",
    "            except OSError:\n",
    "                pass\n",
    "            count = 0\n",
    "            chunk_size = min(max(length, 1), 1024 * 1024)\n",
    "            max_chunks = int(math.ceil(length / chunk_size))\n",
    "            time_initial = time.time()\n",
    "            with open(filename, \"wb\") as out_file:\n",
    "                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n",
    "                    out_file.write(data)\n",
    "                    if not quiet:\n",
    "                        count = count + 1\n",
    "                        time_elapsed = time.time() - time_initial\n",
    "                        download_speed = get_speed(\n",
    "                            time_elapsed, count * chunk_size\n",
    "                        )\n",
    "                        output_progress(\n",
    "                            count, max_chunks, status=download_speed\n",
    "                        )\n",
    "            if not quiet:\n",
    "                print()\n",
    "        except HTTPError as e:\n",
    "            print(\"HTTP error {0}, {1}\".format(e.code, e.reason))\n",
    "        except URLError as e:\n",
    "            print(\"URL error: {0}\".format(e.reason))\n",
    "        except IOError:\n",
    "            raise\n",
    "\n",
    "\n",
    "def cmr_filter_urls(search_results):\n",
    "    \"\"\"Select only the desired data files from CMR response.\"\"\"\n",
    "    if \"feed\" not in search_results or \"entry\" not in search_results[\"feed\"]:\n",
    "        return []\n",
    "\n",
    "    entries = [\n",
    "        e[\"links\"] for e in search_results[\"feed\"][\"entry\"] if \"links\" in e\n",
    "    ]\n",
    "    # Flatten \"entries\" to a simple list of links\n",
    "    links = list(itertools.chain(*entries))\n",
    "\n",
    "    urls = []\n",
    "    unique_filenames = set()\n",
    "    for link in links:\n",
    "        if \"href\" not in link:\n",
    "            # Exclude links with nothing to download\n",
    "            continue\n",
    "        if \"inherited\" in link and link[\"inherited\"] is True:\n",
    "            # Why are we excluding these links?\n",
    "            continue\n",
    "        if \"rel\" in link and \"data#\" not in link[\"rel\"]:\n",
    "            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n",
    "            continue\n",
    "\n",
    "        if \"title\" in link and \"opendap\" in link[\"title\"].lower():\n",
    "            # Exclude OPeNDAP links--they are responsible for many duplicates\n",
    "            # This is a hack; when the metadata is updated to properly identify\n",
    "            # non-datapool links, we should be able to do this in a non-hack way\n",
    "            continue\n",
    "\n",
    "        filename = link[\"href\"].split(\"/\")[-1]\n",
    "        if filename in unique_filenames:\n",
    "            # Exclude links with duplicate filenames (they would overwrite)\n",
    "            continue\n",
    "        unique_filenames.add(filename)\n",
    "\n",
    "        urls.append(link[\"href\"])\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def cmr_search(\n",
    "    short_name,\n",
    "    version,\n",
    "    time_start,\n",
    "    time_end,\n",
    "    bounding_box=\"\",\n",
    "    polygon=\"\",\n",
    "    filename_filter=\"\",\n",
    "    quiet=False,\n",
    "):\n",
    "    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n",
    "    cmr_query_url = build_cmr_query_url(\n",
    "        short_name=short_name,\n",
    "        version=version,\n",
    "        time_start=time_start,\n",
    "        time_end=time_end,\n",
    "        bounding_box=bounding_box,\n",
    "        polygon=polygon,\n",
    "        filename_filter=filename_filter,\n",
    "    )\n",
    "    if not quiet:\n",
    "        print(\"Querying for data:\\n\\t{0}\\n\".format(cmr_query_url))\n",
    "\n",
    "    cmr_scroll_id = None\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    urls = []\n",
    "    hits = 0\n",
    "    while True:\n",
    "        req = Request(cmr_query_url)\n",
    "        if cmr_scroll_id:\n",
    "            req.add_header(\"cmr-scroll-id\", cmr_scroll_id)\n",
    "        response = urlopen(req, context=ctx)\n",
    "        if not cmr_scroll_id:\n",
    "            # Python 2 and 3 have different case for the http headers\n",
    "            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n",
    "            cmr_scroll_id = headers[\"cmr-scroll-id\"]\n",
    "            hits = int(headers[\"cmr-hits\"])\n",
    "            if not quiet:\n",
    "                if hits > 0:\n",
    "                    print(\"Found {0} matches.\".format(hits))\n",
    "                else:\n",
    "                    print(\"Found no matches.\")\n",
    "        search_page = response.read()\n",
    "        search_page = json.loads(search_page.decode(\"utf-8\"))\n",
    "        url_scroll_results = cmr_filter_urls(search_page)\n",
    "        if not url_scroll_results:\n",
    "            break\n",
    "        if not quiet and hits > CMR_PAGE_SIZE:\n",
    "            print(\".\", end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        urls += url_scroll_results\n",
    "\n",
    "    if not quiet and hits > CMR_PAGE_SIZE:\n",
    "        print()\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# washington bounding box\n",
    "bounding_box_str = \"-125,45,-115,49.5\"\n",
    "min_lon, min_lat, max_lon, max_lat = (-125, 45, -115, 49.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up credentials for earthdata login\n",
    "import base64\n",
    "\n",
    "username = \"\"\n",
    "password = \"\"\n",
    "credentials = \"{0}:{1}\".format(username, password)\n",
    "credentials = base64.b64encode(credentials.encode(\"ascii\")).decode(\"ascii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_urls(short_name, version, bounding_box_str):\n",
    "    urls = glas_download.cmr_search(\n",
    "        short_name,\n",
    "        version,\n",
    "        glas_download.time_start,\n",
    "        glas_download.time_end,\n",
    "        bounding_box=bounding_box_str,\n",
    "        polygon=glas_download.polygon,\n",
    "        filename_filter=glas_download.filename_filter,\n",
    "    )\n",
    "    urls = [url for url in urls if url.endswith(\".H5\")]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def download(url, credentials):\n",
    "    glas_download.cmr_download(\n",
    "        [url], credentials=credentials, force=False, quiet=True\n",
    "    )\n",
    "    glas_download.cmr_download(\n",
    "        [url + \".MET.xml\"], credentials=credentials, force=False, quiet=True\n",
    "    )\n",
    "    filename = url.split(\"/\")[-1]\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "def convert_long3_to_long1(long3):\n",
    "    # see https://confluence.ecmwf.int/pages/viewpage.action?pageId=149337515\n",
    "    long1 = (long3 + 180) % 360 - 180\n",
    "    return long1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get all urls for GLAH14 and filter to data within the bounding box\n",
    "# short_name = 'GLAH14'\n",
    "# version = '034'\n",
    "# urls = search_for_urls(short_name, version, bounding_box_str)\n",
    "\n",
    "# # extract data from these urls\n",
    "# data = []\n",
    "# for url in urls:\n",
    "#     # download the H5 file as well as metadata, return the filename\n",
    "#     filename = download(url, credentials)\n",
    "#     temp = extract_GLAH14_data(filename)\n",
    "#     # convert longitude units from 0-360 to -180 to 180\n",
    "#     temp['lon'] = convert_long3_to_long1(temp.lon)\n",
    "#     temp = temp.where((temp.lat > min_lat) & (temp.lat < max_lat) &\n",
    "#                       (temp.lon > min_lon) & (temp.lon < max_lon), drop=True)\n",
    "#     data.append(temp)\n",
    "#     os.remove(filename)\n",
    "#     os.remove(filename+'.MET.xml')\n",
    "\n",
    "# data = xr.concat(data, dim='record_index')\n",
    "# data.to_zarr('../data/glas/washington/GLAH14.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data14 = xr.open_zarr(\"../data/glas/washington/GLAH14.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique list of record index from all GLAH14 data\n",
    "record_indices = np.unique(data14.record_index.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(record_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_name = 'GLAH01'\n",
    "# version = '033'\n",
    "# urls = search_for_urls(short_name, version, bounding_box_str)\n",
    "# data01 = []\n",
    "\n",
    "# for url in urls:\n",
    "#     filename = download(url, credentials)\n",
    "#     temp = extract_GLAH01_data(filename)\n",
    "#     temp = temp.where(temp.record_index.isin(record_indices), drop=True)\n",
    "#     data01.append(temp)\n",
    "\n",
    "#     os.remove(filename)\n",
    "#     os.remove(filename+'.MET.xml')\n",
    "\n",
    "# data01 = xr.concat(data01, dim='record_index')\n",
    "# data01.to_zarr('../data/glas/washington/GLAH01.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data01 = xr.open_zarr(\"../data/glas/washington/GLAH01.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = data14.merge(data01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
