{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landsat Processing\n",
    "Created by: Oriana Chegwidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import boto3\n",
    "from rasterio.session import AWSSession\n",
    "aws_session = AWSSession(boto3.Session(), requester_pays=True)\n",
    "from osgeo.gdal import VSICurlClearCache\n",
    "VSICurlClearCache() \n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import dask\n",
    "import os\n",
    "import fsspec\n",
    "from satsearch import Search\n",
    "from matplotlib.pyplot import imshow\n",
    "from intake import open_stac_item_collection\n",
    "import osgeo\n",
    "import numcodecs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f724e481ef649b98ae93ed52e17fd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>GatewayCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n<style scoped>\\n    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dask_gateway import Gateway\n",
    "\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.worker_cores = 2\n",
    "options.worker_memory = 32\n",
    "options.environment = {'AWS_REQUEST_PAYER': 'requester'}\n",
    "cluster = gateway.new_cluster(cluster_options=options)\n",
    "cluster.adapt(minimum=1, maximum=10)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Landsat scene is stored in cloud optimized geotiff (COG) according to a verbose (but once you understand it, human readable!) naming convention. Landsat Collection 2 uses the same naming convention as Collection 1 which is as follows (lifted from their docs at `https://prd-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/atoms/files/LSDS-1656_%20Landsat_Collection1_L1_Product_Definition-v2.pdf`\n",
    "\n",
    "```LXSS_LLLL_PPPRRR_YYYYMMDD_yyyymmdd_CC_TX```\n",
    "where\n",
    "```\n",
    "L = Landsat  (constant)\n",
    "X = Sensor  (C = OLI / TIRS, O = OLI-only, T= TIRS-only, E = ETM+, T = TM, M= MSS)\n",
    "SS = Satellite  (e.g., 04 for Landsat 4, 05 for Landsat 5, 07 for Landsat 7, etc.) \n",
    "LLLL = Processing  level  (L1TP, L1GT, L1GS)\n",
    "PPP  = WRS path\n",
    "RRR  = WRS row\n",
    "YYYYMMDD = Acquisition  Year (YYYY) / Month  (MM) / Day  (DD) \n",
    "yyyymmdd  = Processing  Year (yyyy) / Month  (mm) / Day (dd)\n",
    "CC = Collection  number  (e.g., 01, 02, etc.) \n",
    "TX= RT for Real-Time, T1 for Tier 1 (highest quality), and T2 for Tier 2\n",
    "\n",
    "```\n",
    "\n",
    "Thus, we're looking for scenes coded in the following way:\n",
    "`LE07_????_PPP_RRR_YYYMMDD_yyyymmdd_02_T1` for Landsat 7 and\n",
    "`LT05_????_PPP_RRR_YYYMMDD_yyyymmdd_02_T1` for Landsat 5\n",
    "(but T1 might be wrong there)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are re-implementing (to the best of our abilities) the methods from Wang et al (in review). Jon Wang's paper said:\n",
    "\n",
    "```To extend our AGB predictions through space and time, we used time series (1984 – 2014) of 30 m surface reflectance data from the Thematic Mapper onboard Landsat 5 and the Enhanced Thematic Mapper Plus onboard Landsat 7. We used the GLAS-derived estimates of AGB as a response variable and the mean growing season (June, July, August) and non-growing season values for each of Landsat’s six spectral reflectance bands as the predictors in an ensemble machine learning model```\n",
    "\n",
    "So we'll be looking for:\n",
    "* Landsat 5 (Thematic mapper) and 7 (Enhanced Thematic Mapper Plus)\n",
    "* Growing season (June-August) and non-growing season (Sept-May) averages at an annual timestep. <--- will need to figure out around the calendar whether we want consecutive\n",
    "* All six spectral reflectance bands\n",
    "* We'll do a quality thresholding of cloudless cover for now based upon their thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In orienting myeslf, these are the potential collection options I've figured out (by poking around here on the [sat-api catalog](https://landsatlook.usgs.gov/sat-api/collections):\n",
    "* `landsat-c2l2-sr` Landsat Collection 2 Level-2 UTM Surface Reflectance (SR) Product\n",
    "* `landsat-c2l2alb-sr` Landsat Collection 2 Level-2 Albers Surface Reflectance (SR) Product\n",
    "* `landsat-c1l2alb-sr` Landsat Collection 1 Level-2 Albers Surface Reflectance (SR) Product <-- we don't want this one (b/c we'll go with collection 2)\n",
    "* `landsat-c2l1` Landsat Collection 2 Level-1 Product <-- don't think we want this because we want surface reflectance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this once to apply the aws session to the rasterio environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_worker_credentials(aws_session):    \n",
    "#     VSICurlClearCache()\n",
    "    # this file is the canary in the coal mine\n",
    "    # if you can't open this one you've got *issues* because it exists!\n",
    "    # also the instantiation of the environment here\n",
    "    # might help you turn on the switch of the credentials\n",
    "    # but maybe that's just anecdotal i hate credential stuff SO MUCH\n",
    "    # if anyone is reading this message i hope you're enjoying my typing\n",
    "    # as i wait for my cluster to start up.... hmm....\n",
    "    canary_file = 's3://usgs-landsat/collection02/level-2/standard/tm/2003/044/029/LT05_L2SP_044029_20030827_20200904_02_T1/LT05_L2SP_044029_20030827_20200904_02_T1_SR_B2.TIF'\n",
    "\n",
    "    with rio.Env(aws_session):\n",
    "        with rio.open(canary_file) as src:\n",
    "            profile = src.profile\n",
    "            arr = src.read(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_catalog(bbox, time_slice):\n",
    "    url = 'https://landsatlook.usgs.gov/sat-api/stac/'\n",
    "    search = Search.search(url=url,\n",
    "                           bbox=bbox, \n",
    "                           time=time_slice,\n",
    "                           # for some reason the collection subsetting isn't really working and \n",
    "                           # I'm still getting landsat l1 stuff- shrug. we can filter elsewhere\n",
    "                           collection=['landsat-c2l2-sr'])\n",
    "    items = search.items(limit=7000)\n",
    "    summary = items.summary()\n",
    "    catalog = open_stac_item_collection(items)\n",
    "    return catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what is difference between Item and Collection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a single one of those files and loading it looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_link(url):\n",
    "    return url.replace('https://landsatlook.usgs.gov/data', 's3://usgs-landsat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different kinds of QA/QC bands contained in L2SP:\n",
    "* SR_CLOUD_QA - I think we want this one because anything less than 2 is either just dark dense vegetation or no flags. everything above is stuff like water, snow, cloud (different levels of obscurity). This is the result of the fmask algorithm from Zhu et al.\n",
    "* QA_PIXEL - this gets a little more specific and goes intot different kinds of clouds. Super interesting but I don't think we want to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in the SR_CLOUD_QA and use as a mask - see Table 5-3 in https://prd-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/atoms/files/LSDS-1370_L4-7_C1-SurfaceReflectance-LEDAPS_ProductGuide-v3.pdf for description of cloud integer values to select which ones to use as drop. For now I'll drop anything greater than 1 (0= no QA concerns and 1 is Dark dense vegetation (DDV))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_qa(item):\n",
    "    import numpy as np\n",
    "    qa_path = fix_link(item._stac_obj.assets['SR_CLOUD_QA.TIF']['href'])\n",
    "    cog_mask = xr.open_rasterio(qa_path).squeeze().drop('band')\n",
    "    return cog_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_ds(item, bands_of_interest, cog_mask):\n",
    "    url_list = [fix_link(item._stac_obj.assets['{}.TIF'.format(band)]['href']) for band in bands_of_interest]\n",
    "    da_list = []\n",
    "    for url in url_list:\n",
    "        da_list.append(xr.open_rasterio(url, chunks={'x': 1280,\n",
    "                                                    'y': 1280}))\n",
    "    # combine into one dataset\n",
    "    ds = xr.concat(da_list, dim='band').to_dataset(dim='band').rename({1: 'reflectance'})\n",
    "    ds = ds.assign_coords({'band': bands_of_interest})\n",
    "    ds = ds.where(cog_mask<2).compute()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make the query using sat-search to find every file in the STAC catalog that we want. We'll store that list of files. We'll do this first for a single tile (in this first exmaple just covering Washington State) but then we'll loop through in 1-degree by 1-degree tiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_list(catalog, aws_session, bands_of_interest='all'):\n",
    "    if bands_of_interest=='all':\n",
    "        bands_of_interest = ['SR_B1', 'SR_B2', 'SR_B3', \n",
    "                                 'SR_B4', 'SR_B5', 'SR_B7']\n",
    "    ds_list = []\n",
    "    for scene_id in list(catalog):\n",
    "        # only grab the l2sp and landsat5 for now. for some reason\n",
    "        # satsearch is having difficulty filtering out these.)\n",
    "        if ('L2SP' in scene_id) and ('LT05' in scene_id):\n",
    "            item = catalog[scene_id]\n",
    "            cog_mask = cloud_qa(item)\n",
    "            ds = grab_ds(item, bands_of_interest, cog_mask)\n",
    "            ds_list.append(ds)\n",
    "    return ds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(ds_list):\n",
    "    full_ds = xr.concat(ds_list, dim='scene').mean(dim='scene').compute()\n",
    "    full_ds = full_ds.chunk({'band': 1, 'x': 1280, 'y': 1280})\n",
    "    return full_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(ds, mapper, aws_session):\n",
    "    encoding = {'reflectance': {'compressor': numcodecs.Blosc()}}\n",
    "    with rio.Env(aws_session):\n",
    "#         with dask.config.set(scheduler='threads'): # this?\n",
    "        ds.to_zarr(store=mapper,\n",
    "                        encoding=encoding, \n",
    "                         mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dask.delayed # this?\n",
    "def process_tile(bbox, time_slice, mapper, aws_session, bands_of_interest='all'):\n",
    "    # This is the workflow we'll actually pass off to the worker.\n",
    "    with rio.Env(aws_session):\n",
    "        test_worker_credentials(aws_session)\n",
    "        catalog = create_catalog(bbox, time_slice)\n",
    "        ds_list = create_dataset_list(catalog, aws_session, bands_of_interest=bands_of_interest)\n",
    "        full_ds = combine(ds_list)\n",
    "        write_out(full_ds, mapper, aws_session)\n",
    "    return 'Completed processing of {} for region bounded by '\\\n",
    "            '{} over time period {}'.format(bands_of_interest, bbox, time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PANGEO_SCRATCH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c5f208822efc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ms3fs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3fs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS3FileSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPANGEO_SCRATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'PANGEO_SCRATCH' is not defined"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem()\n",
    "fs.ls(PANGEO_SCRATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(2003,2009)\n",
    "bands = ['SR_B1', 'SR_B2', 'SR_B3', \n",
    "        'SR_B4', 'SR_B5', 'SR_B7']\n",
    "tile_bboxes = []\n",
    "lat_box_length = 2\n",
    "lon_box_length = 1\n",
    "for lat1 in np.arange(45,49,lat_box_length):\n",
    "    for lon1 in np.arange(-125,-117,lon_box_length):\n",
    "        tile_bboxes.append([lon1, lat1, lon1+lon_box_length, lat1+lat_box_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dask.config.set({\"array.slicing.split_large_chunks\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "bbox = tile_bboxes[0]\n",
    "lon1, lat1 = bbox[0], bbox[1] \n",
    "year = 2003\n",
    "PANGEO_SCRATCH=os.environ['PANGEO_SCRATCH']\n",
    "\n",
    "timeslice =f\"{year}-06-01T00:00:00Z/{year}-08-31T23:59:59Z\"\n",
    "# file_save_template = 'landsat5/{}/{}/'\n",
    "mapper = fsspec.get_mapper(f'{PANGEO_SCRATCH}test810.zarr')\n",
    "band = ['SR_B1']\n",
    "url = f'{PANGEO_SCRATCH}{lat1}_{lon1}/{year}/{band[0]}.zarr'\n",
    "mapper = fsspec.get_mapper(url)\n",
    "\n",
    "# years = np.arange(2003,2009)\n",
    "# i=0\n",
    "# for [lon1, lat1, lon2, lat2] in tile_bboxes:\n",
    "#     for year in years:        \n",
    "#         for band in bands:\n",
    "#             time_slice=\n",
    "#             i+=1\n",
    "task = process_tile([-125, 45, -124, 46], #bbox\n",
    "            \"2003-06-01T00:00:00Z/2003-08-31T23:59:59Z\", #timeslice, \n",
    "             mapper, \n",
    "            aws_session,\n",
    "            bands_of_interest=band)\n",
    "# # dask.compute(task, retries=1)\n",
    "# out.append(dask.compute(task, retries=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the list of files for a given year to average across growing season and non-growing season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cog(filepath):\n",
    "    '''\n",
    "    Function for hard restart read. If you have trouble reading things\n",
    "    try this approach to ensure nothing else is funky.\n",
    "    '''\n",
    "    # this fixes it - but how do we implement in rasterio?\n",
    "    VSICurlClearCache()\n",
    "# does applying the aws_session in the env once just set it permannently?\n",
    "    with rio.Env(aws_session):\n",
    "        with rio.open(filepath) as src:\n",
    "            profile = src.profile\n",
    "            arr = src.read(1)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_file = 's3://usgs-landsat/collection02/level-2/standard/tm/2003/047/027/LT05_L2SP_047027_20030629_20200904_02_T1/LT05_L2SP_047027_20030629_20200904_02_T1_SR_B1.TIF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_file = 's3://usgs-landsat/collection02/level-2/standard/tm/2003/044/029/LT05_L2SP_044029_20030827_20200904_02_T1/LT05_L2SP_044029_20030827_20200904_02_T1_SR_B2.TIF'\n",
    "broken_file = 's3://usgs-landsat/collection02/level-2/standard/tm/2003/044/029/LT05_L2SP_044029_20030827_20200904_02_T1/LT05_L2SP_044029_20030827_20200904_02_T1_SR_B8.TIF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = rio.Env(aws_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_cog(mean_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
