{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcsfs import GCSFileSystem\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "fs = GCSFileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in carbonplan-scratch\n",
    "files = fs.ls('carbonplan-scratch/glas-cache')\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../extract_GLAS_data.py\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def read_dimensions(file_handle):\n",
    "    # put the dimension columns into a dataframe\n",
    "    df = {}\n",
    "    df['record_index'] = file_handle['Data_40HZ/Time/i_rec_ndx'][:]\n",
    "    df['shot_number'] = file_handle['Data_40HZ/Time/i_shot_count'][:]\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    # concat two columns to get an unique index, but also set these two columns as multi level index \n",
    "    df['unique_index'] = df.record_index.astype(str).str.zfill(9) + '_' + df.shot_number.astype(str).str.zfill(2)\n",
    "    df.set_index(['record_index', 'shot_number'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_1d_variables(file_handle, mapping, unique_index, replace_fill_values_with_nulls):\n",
    "    # put the 1D variables into a xarray dataset \n",
    "    ds = {}\n",
    "    for k, v in mapping.items():\n",
    "        temp = file_handle[v][:]\n",
    "        if replace_fill_values_with_nulls:\n",
    "            temp[(temp > 1e+100)] = np.nan\n",
    "        ds[k] = xr.DataArray(temp, dims=['unique_index'], coords={'unique_index': unique_index})\n",
    "    \n",
    "    return xr.Dataset(ds)\n",
    "    \n",
    "    \n",
    "def extract_GLAH01_data(filename, replace_fill_values_with_nulls=True):\n",
    "    \"\"\"\n",
    "    Given file name of a HDF5 GLAH01 data, returns a xarray dataset with record index and shot number being the primary dimensions. \n",
    "    rec_bin and tx_bin are additional dimensions regarding the transmitted and received waveforms. \n",
    "    \"\"\"\n",
    "    \n",
    "    # read file\n",
    "    f = h5py.File(filename, 'r')\n",
    "    \n",
    "    # list out all 1D variables we want and read into a dataframe \n",
    "    name_map = {\n",
    "        # Background Noise Mean Value for the 4 ns filter. From APID12/13, Offset 112.\n",
    "        'noise_mean': 'Data_40HZ/Waveform/Characteristics/d_4nsBgMean',  # volts \n",
    "        # The standard deviation of the background noise for the 4 ns filter. From APID12/13, Offset 116.\n",
    "        'noise_sd': 'Data_40HZ/Waveform/Characteristics/d_4nsBgSDEV',  # volts \n",
    "        'rec_wf_location_ind': 'Data_40HZ/Waveform/RecWaveform/i_rec_wf_location_index',  # This is an index into the array of 544 times within the rec_wf_sample_location_table (found in the ANCILLARY_DATA group)\n",
    "        'rec_wf_response_end_time': 'Data_40HZ/Waveform/RecWaveform/i_RespEndTime',\n",
    "        'tx_wf_peak_time': 'Data_40HZ/Waveform/TransmitWaveform/i_time_txWfPk',  # Address in digitizer counts of the Transmit Pulse Peak as measured from the start of Acquisition Memory, i.e. start of digitization. From APID12/13, Offset 68.\n",
    "#     'wf_type': 'Data_40HZ/Waveform/Characteristics/i_waveformType', # Indicates number of valid samples in waveform; 0 = missing; 1 = Long waveform (544 samples); 2 =Short waveform (200 samples),\n",
    "#     'tx_wf_start_time': 'Data_40HZ/Waveform/TransmitWaveform/i_TxWfStart'  # Starting Address in digitizer counts of the Transmit Pulse sample relative to the start of digitization. From APID12/13, Offset 76.\t\n",
    "    }\n",
    "    \n",
    "    # put the dimension columns into a dataframe\n",
    "    df = read_dimensions(f)\n",
    "    \n",
    "    # put the 1D variables into a xarray dataset \n",
    "    ds = read_1d_variables(file_handle=f, mapping=name_map, unique_index=df.unique_index.values, replace_fill_values_with_nulls=False)\n",
    "\n",
    "    # read the 2D variables we want \n",
    "    # Transmit Pulse 48 waveform samples in calibrated volts. The delta times for transmit waveform sample j is provided in the attribute array tx_wf_sample_location_table (j).\n",
    "    tx_wf = f['Data_40HZ/Waveform/TransmitWaveform/r_tx_wf'][:]\n",
    "    # The delta times for each echo of the 544 waveform samples is provided within the 544 times stored in rec_wf_sample_location_table \n",
    "    # (an attribute in the /ANCILLARY_DATA group) and indexed by i_rec_wf_location_index.\n",
    "    rec_wf = f['Data_40HZ/Waveform/RecWaveform/r_rng_wf'][:]  # n (num shot * num records) x 544 \n",
    "    tx_wf_sample_loc = f['ANCILLARY_DATA'].attrs['tx_wf_sample_location_table']\n",
    "    rec_wf_sample_loc = f['ANCILLARY_DATA'].attrs['rec_wf_sample_location_table']  # 5 x 544\n",
    "    \n",
    "    if replace_fill_values_with_nulls:\n",
    "        rec_wf_sample_loc[(rec_wf_sample_loc > 1e+10)] = np.nan\n",
    "        \n",
    "    # put the 2D variables into xarray \n",
    "    ds['rec_wf'] = xr.DataArray(rec_wf, dims=['unique_index', 'rec_bin'], coords=[df.unique_index.values, np.arange(rec_wf.shape[1])])\n",
    "    ds['tx_wf'] = xr.DataArray(tx_wf, dims=['unique_index', 'tx_bin'], coords=[df.unique_index.values, np.arange(tx_wf.shape[1])])\n",
    "    ds['tx_wf_sample_loc'] = xr.DataArray(tx_wf_sample_loc, dims=['tx_bin'], coords=[np.arange(tx_wf.shape[1])])\n",
    "\n",
    "    # store a copy of the sample location for each unique shot \n",
    "    ds['rec_wf_sample_loc'] = xr.DataArray(rec_wf_sample_loc[ds.rec_wf_location_ind - 1], dims=['unique_index', 'rec_bin'], coords=[df.unique_index.values, np.arange(rec_wf.shape[1])])\n",
    "    \n",
    "    # expand the multi index \n",
    "    ds.coords['unique_index'] = df.index\n",
    "    ds = ds.unstack('unique_index')\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def extract_GLAH14_data(filename, replace_fill_values_with_nulls=True):\n",
    "    \"\"\"\n",
    "    Given file name of a HDF5 GLAH14 data, returns a xarray dataset with record index and shot number being the primary dimensions. \n",
    "    rec_bin and tx_bin are additional dimensions regarding the transmitted and received waveforms. \n",
    "    \"\"\"\n",
    "    \n",
    "    # read file\n",
    "    f = h5py.File(filename, 'r')\n",
    "    \n",
    "    # list out all 1D variables we want and read into a dataframe \n",
    "    name_map = {\n",
    "        # The transmit time of each shot in the 1 second frame measured as UTC seconds elapsed since Jan 1 2000 12:00:00 UTC. This time has been derived from the GPS time accounting for leap seconds.\n",
    "        'time': 'Data_40HZ/Time/d_UTCTime_40',  \n",
    "        'lat': 'Data_40HZ/Geolocation/d_lat',\n",
    "        'lon': 'Data_40HZ/Geolocation/d_lon',\n",
    "        # the documentation mentioned two flags sat_corr_flg and i_satNdx to signal bad elevation, also and when correction is invalid the elevation is invalid \n",
    "        'elevation': 'Data_40HZ/Elevation_Surfaces/d_elev',  # meters \n",
    "        'elevation_correction': 'Data_40HZ/Elevation_Corrections/d_satElevCorr',  # should be added to elevation \n",
    "        # Range in distance calculated from the time between the centroid of the transmit pulse and the farthest gate from the spacecraft of the received pulse. See the rngcorrflg to determine \n",
    "        # any corrections that have been applied. unit is meters and values in the 600k range \n",
    "        'ref_range': 'Data_40HZ/Elevation_Surfaces/d_refRng',  # meters \n",
    "        # these should be added to centroid according to the documentation \n",
    "        'sig_begin_offset': 'Data_40HZ/Elevation_Offsets/d_SigBegOff',  # meters \n",
    "        'sig_end_offset': 'Data_40HZ/Elevation_Offsets/d_SigEndOff',  # meters \n",
    "        # Range offset to be added to d_refRng to calculate the range using the algorithm deemed appropriate for land.\n",
    "        'centroid_offset': 'Data_40HZ/Elevation_Offsets/d_ldRngOff',  # meters \n",
    "        # data for the 6 fitted gaussian peaks \n",
    "#         'num_gaussian_peaks': 'Data_40HZ/Waveform/i_numPk', \n",
    "#         'gaussian_mu': 'Data_40HZ/Elevation_Offsets/d_gpCntRngOff',  # meters \n",
    "#         'gaussian_amp': 'Data_40HZ/Waveform/d_Gamp',  # volts \n",
    "#         'gaussian_sigma': 'Data_40HZ/Waveform/d_Gsigma',  # ns\n",
    "    }\n",
    "    \n",
    "    # put the dimension columns into a dataframe\n",
    "    df = read_dimensions(f)\n",
    "    \n",
    "    # put the 1D variables into a xarray dataset \n",
    "    ds = read_1d_variables(file_handle=f, mapping=name_map, unique_index=df.unique_index.values, replace_fill_values_with_nulls=replace_fill_values_with_nulls)\n",
    "    \n",
    "    # expand the multi index \n",
    "    ds.coords['unique_index'] = df.index\n",
    "    ds = ds.unstack('unique_index')\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all the files\n",
    "ds_01_list = []\n",
    "ds_14_list = []\n",
    "\n",
    "for uri in files:\n",
    "    \n",
    "    # skip xml files\n",
    "    if uri.endswith('xml'):\n",
    "        continue\n",
    "    \n",
    "    # load 01 files\n",
    "    if 'GLAH01' in uri:\n",
    "        print(uri, '\\n')\n",
    "        with fs.open(uri) as f:\n",
    "            ds = extract_GLAH01_data(f)\n",
    "            ds_ds_01_list14_list.append(ds)\n",
    "        display(ds)\n",
    "        \n",
    "    # load 14 files\n",
    "    elif 'GLAH14' in uri:\n",
    "        print(uri, '\\n')\n",
    "        with fs.open(uri) as f:\n",
    "            ds = extract_GLAH14_data(f)\n",
    "            ds_14_list.append(ds)\n",
    "        display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-dimension",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
